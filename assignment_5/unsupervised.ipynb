{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение без учителя: уменьшение размерности и кластеризация\n",
    "\n",
    "Основное отличие методов обучения без учителя от привычных классификаций и регрессий машинного обучения в том, что разметки для данных в этом случае нет. От этого образуются сразу несколько особенностей — во-первых это возможность использования несопоставимо больших объёмов данных, поскольку их не нужно будет размечать руками для обучения, а во-вторых это неясность измерения качества методов, из-за отсутствия таких же прямолинейных и интуитивно понятных метрик, как в задачах обучения с учителем.\n",
    "\n",
    "Одна из возможных задач — это понижение размерности в даных, например, для визуализации. С другой стороны, такая задача позволяет убрать из наблюдений сильно скоррелированные признаки, что облегчит дальнейшее построение модели.\n",
    "\n",
    "## Метод главных компонент (PCA)\n",
    "\n",
    "Метод главных компонент — один из самых часто используемых подходов для снижения размерности данных.\n",
    "\n",
    "В совсем общем виде это можно представить как предположение о том, что все наши наблюдения скорее всего выглядят как некий эллипсоид в подпространстве нашего исходного пространства и наш новый базис в этом пространстве совпадает с осями этого эллипсоида. Это предположение позволяет нам одновременно избавиться от сильно скоррелированных признаков, так как вектора базиса пространства, на которое мы проецируем, будут ортогональными.\n",
    "\n",
    "<center>\n",
    "    <img width=500 src='images/pca.png'>   \n",
    "</center>\n",
    "\n",
    "В общем случае размерность этого эллипсоида будет равна размерности исходного пространства, но наше предположение о том, что данные лежат в подпространстве меньшей размерности, позволяет нам отбросить \"лишнее\" подпространство в новой проекции, а именно то подпространство, вдоль осей которого эллипсоид будет наименее растянут. Мы будем это делать \"жадно\", выбирая по-очереди в качестве нового элемента базиса нашего нового подпространства последовательно ось эллипсоида из оставшихся, вдоль которой дисперсия будет максимальной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим `PCA` к ирисам, чтобы отобразить их на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2, random_state=7)\n",
    "X_centered = X - X.mean(axis=0)\n",
    "pca.fit(X_centered)\n",
    "X_pca = pca.transform(X_centered)\n",
    "\n",
    "plt.plot(X_pca[y == 0, 0], X_pca[y == 0, 1], 'bo', label='Setosa')\n",
    "plt.plot(X_pca[y == 1, 0], X_pca[y == 1, 1], 'go', label='Versicolour')\n",
    "plt.plot(X_pca[y == 2, 0], X_pca[y == 2, 1], 'ro', label='Virginica')\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PCA` так же можно использовать перед обучением моделей, чтобы \"облегчить\" им поиск знакономерностей в данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y, random_state=7, shuffle=True)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=7)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy before PCA: {:.5f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=.3, stratify=y, random_state=7, shuffle=True)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=7)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy after PCA: {:.5f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что качество возросло незначительно, но для более сложных данных более высокой размерности, где данные не разбиваются тривиально вдоль одного признака, применение PCA может достаточно сильно улучшить качество работы деревьев решений и ансамблей на их основе.\n",
    "\n",
    "`sklearn` позволяет заглянуть внутрь модели и посмотреть, какой процент диспресии объясняет каждая компонента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, component in enumerate(pca.components_):\n",
    "    print(f\"{i + 1} component: {round(100 * pca.explained_variance_ratio_[i], 2)}% of initial variance\")\n",
    "    print(\"this component is combination of:\")\n",
    "    print(\" + \".join(f\"{round(value, 3)} x {name}\" for value, name in zip(component, iris.feature_names)))\n",
    "    print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рукописные цифры\n",
    "\n",
    "Посмотрим на чуть более сложный пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X[i,:].reshape([8,8]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# исходная размерность -- 64, снизим до 2\n",
    "\n",
    "pca = decomposition.PCA(n_components=2, random_state=7)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor='none', alpha=0.7, s=40, cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативой к `PCA` может служить алгоритм `t-SNE`. Зачастую он работает лучше, но и дольше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(random_state=7)\n",
    "\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y,edgecolor='none', alpha=0.7, s=40, cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике, как правило, выбирают столько главных компонент, чтобы оставить 90% дисперсии исходных данных. В данном случае для этого достаточно выделить 21 главную компоненту, то есть снизить размерность с 64 признаков до 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(random_state=7)\n",
    "pca.fit(X)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.xlim(0, 63)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.axvline(21, c='b')\n",
    "plt.axhline(0.9, c='r')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация\n",
    "\n",
    "Интуитивная постановка задачи кластеризации довольно проста и представляет из себя наше желание сказать: \"Вот тут у меня насыпаны точки. Я вижу, что они сваливаются в какие-то кучки вместе. Было бы круто иметь возможность эти точки относить к кучкам и в случае появления новой точки на плоскости говорить, в какую кучку она падает.\" Из такой постановки видно, что пространства для фантазии получается много, и от этого возникает соответствующее множество алгоритмов решения этой задачи. Вот несколько самых популярных:\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/clusterings.png\">\n",
    "</center>\n",
    "\n",
    "## K-means\n",
    "\n",
    "Самый популярный и простой алгоритм кластеризации, его очень легко представить в виде последовательности шагов:\n",
    "\n",
    "1. Выбрать количество кластеров $k$, которое нам кажется оптимальным для наших данных.\n",
    "2. Высыпать случайным образом в пространство наших данных $k$ точек (центроидов).\n",
    "3. Для каждой точки нашего набора данных посчитать, к какому центроиду она ближе.\n",
    "4. Переместить каждый центроид в центр выборки, которую мы отнесли к этому центроиду.\n",
    "5. Повторять последние два шага фиксированное число раз, либо до тех пор пока центроиды не \"сойдутся\" (обычно это значит, что их смещение относительно предыдущего положения не превышает какого-то заранее заданного небольшого значения).\n",
    "\n",
    "В случае обычной евклидовой метрики для точек лежащих на плоскости, этот алгоритм очень просто расписывается аналитически и рисуется. Давайте посмотрим соответствующий пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Начнём с того, что насыпем на плоскость три кластера точек\n",
    "X = np.zeros((150, 2))\n",
    "\n",
    "np.random.seed(seed=7)\n",
    "X[:50, 0] = np.random.normal(loc=0.0, scale=.4, size=50)\n",
    "X[:50, 1] = np.random.normal(loc=0.0, scale=.4, size=50)\n",
    "\n",
    "X[50:100, 0] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "X[50:100, 1] = np.random.normal(loc=-1.0, scale=.3, size=50)\n",
    "\n",
    "X[100:150, 0] = np.random.normal(loc=-1.0, scale=.3, size=50)\n",
    "X[100:150, 1] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(X[:, 0], X[:, 1], 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В scipy есть замечательная функция, которая считает расстояния\n",
    "# между парами точек из двух массивов, подающихся ей на вход\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# насыпем три случайные центроиды для начала\n",
    "np.random.seed(7)\n",
    "centroids = np.random.normal(loc=0.0, scale=1., size=6)\n",
    "centroids = centroids.reshape((3, 2))\n",
    "\n",
    "cent_history = []\n",
    "cent_history.append(centroids)\n",
    "\n",
    "for i in range(3):\n",
    "    # Считаем расстояния от наблюдений до центроид\n",
    "    distances = cdist(X, centroids)\n",
    "    # Смотрим, до какой центроиде каждой точке ближе всего\n",
    "    labels = distances.argmin(axis=1)\n",
    "    \n",
    "    # Положим в каждую новую центроиду геометрический центр её точек\n",
    "    centroids = centroids.copy()\n",
    "    centroids[0, :] = np.mean(X[labels == 0, :], axis=0)\n",
    "    centroids[1, :] = np.mean(X[labels == 1, :], axis=0)\n",
    "    centroids[2, :] = np.mean(X[labels == 2, :], axis=0)\n",
    "    \n",
    "    cent_history.append(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(4):\n",
    "    distances = cdist(X, cent_history[i])\n",
    "    labels = distances.argmin(axis=1)\n",
    "    \n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(X[labels == 0, 0], X[labels == 0, 1], 'bo', label='cluster #1')\n",
    "    plt.plot(X[labels == 1, 0], X[labels == 1, 1], 'co', label='cluster #2')\n",
    "    plt.plot(X[labels == 2, 0], X[labels == 2, 1], 'mo', label='cluster #3')\n",
    "    plt.plot(cent_history[i][:, 0], cent_history[i][:, 1], 'rX')\n",
    "    plt.legend(loc=0)\n",
    "    plt.title('Step {:}'.format(i + 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также стоит заметить, что хоть мы и рассматривали евклидово расстояние, алгоритм будет сходиться и в случае любой другой метрики, поэтому для различных задач кластеризации в зависимости от данных можно экспериментировать не только с количеством шагов или критерием сходимости, но и с метрикой, по которой мы считаем расстояния между точками и центроидами кластеров.\n",
    "\n",
    "Другой особенностью этого алгоритма является то, что он чувствителен к исходному положению центроид кластеров в пространстве. В такой ситуации спасает несколько последовательных запусков алгоритма с последующим усреднением полученных кластеров.\n",
    "\n",
    "## Выбор оптимального числа кластеров (правило \"локтя\")\n",
    "\n",
    "В отличие от задачи классификации или регресии, в случае кластеризации сложнее выбрать критерий, с помощью которого было бы просто представить задачу кластеризации как задачу оптимизации. В случае `kMeans` распространен вот такой критерий – сумма квадратов расстояний от точек до центроидов кластеров, к которым они относятся.\n",
    "\n",
    "$$\n",
    "J(C) = \\sum_{k=1}^K\\sum_{i~\\in~C_k} ||x_i - \\mu_k|| \\rightarrow \\min\\limits_C\n",
    "$$\n",
    "\n",
    "здесь $C$ – множество кластеров мощности $K$, $\\mu_k$ – центроид кластера $C_k$.\n",
    "\n",
    "Понятно, что здравый смысл в этом есть: мы хотим, чтобы точки распологались кучно возле центров своих кластеров. Но у такого подхода есть недостаток: минимум такого фнукционала будет достигаться тогда, когда кластеров столько же, сколько и точек (то есть каждая точка – это кластер из одного элемента). Для решения этого вопроса (выбора числа кластеров) часто пользуются такой эвристикой: выбирают то число кластеров, начиная с которого описанный функционал $ J(C) $ падает \"уже не так быстро\". Или более формально: $$ D(k) = \\frac{|J(C_k) - J(C_{k+1})|}{|J(C_{k-1}) - J(C_k)|}  \\rightarrow \\min\\limits_k $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# kmeans.inertia_ -- сумма квадратов расстояний до ближайшего центроида\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 8):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=7).fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1, 8), inertia, marker='s');\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('$J(C_k)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что $J(C_k)$ падает сильно при увеличении числа кластеров с 1 до 2 и с 2 до 3 и уже не так сильно – при изменении $k$ с 3 до 4. Значит, в данной задаче оптимально задать 3 кластера.\n",
    "\n",
    "### Сложности\n",
    "\n",
    "Само по себе решение задачи `K-means` NP-трудное, и для размерности $d$, числа кластеров $k$ и числа точек $n$ решается за $O(n^{d k+1})$. Для решения такой боли часто используются эвристики, например `MiniBatch K-means`, который для обучения использует не весь датасет целиком, а лишь маленькие его порции (batch) и обновляет центроиды используя среднее за всю историю обновлений центроида от всех относящихся к нему точек. Сравнение обычного `K-means` и его `MiniBatch` имплементации можно посмотреть в документации scikit-learn.\n",
    "\n",
    "Реализация алгоритма в `scikit-learn` обладает массой удобных плюшек, таких как возможность задать количество запусков через параметр `n_init`, что даст более устойчивые центроиды для кластеров в случае скошенных данных. К тому же эти запуски можно делать параллельно, не жертвуя временем вычисления.\n",
    "\n",
    "# Метрики качества кластеризации\n",
    "\n",
    "Выделяют внешние и внутренние метрики качества. Внешние используют информацию об истинном разбиении на кластеры, в то время как внутренние метрики не используют никакой внешней информации и оценивают качество кластеризации, основываясь только на наборе данных. Оптимальное число кластеров обычно определяют с использованием внутренних метрик.\n",
    "\n",
    "Все указанные ниже метрики реализованы в `sklearn.metrics`.\n",
    "\n",
    "## Adjusted Rand Index (ARI)\n",
    "\n",
    "Предполагается, что известны истинные метки объектов. Данная мера не зависит от самих значений меток, а только от разбиения выборки на кластеры. Пусть $N$ - число объектов в выборке. Обозначим через $a$ - число пар объектов, имеющих одинаковые метки и находящихся в одном кластере, через $b$ - число пар объектов, имеющих различные метки и находящихся в разных кластерах. Тогда Rand Index это\n",
    "\n",
    "$$\n",
    "\\text{RI} = \\frac{2(a + b)}{n(n-1)}\n",
    "$$\n",
    "\n",
    "То есть это доля объектов, для которых эти разбиения (исходное и полученное в результате кластеризации) \"согласованы\". Rand Index (RI) выражает схожесть двух разных кластеризаций одной и той же выборки. Чтобы этот индекс давал значения близкие к нулю для случайных кластеризаций при любом $N$ и числе кластеров, необходимо нормировать его. Так определяется Adjusted Rand Index:\n",
    "\n",
    "$$\n",
    "\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}\n",
    "$$\n",
    "\n",
    "Эта мера симметрична, не зависит от значений и перестановок меток. Таким образом, данный индекс является мерой расстояния между различными разбиениями выборки. $\\text{ARI}$ принимает значения в диапазоне $[-1, 1]$. Отрицательные значения соответствуют \"независимым\" разбиениям на кластеры, значения, близкие к нулю, - случайным разбиениям, и положительные значения говорят о том, что два разбиения схожи (совпадают при $\\text{ARI} = 1$).\n",
    "\n",
    "## Adjusted Mutual Information (AMI)\n",
    "\n",
    "Данная мера очень похожа на $\\text{ARI}$. Она также симетрична, не зависит от значений и перестановок меток. Определяется с использованием функции энтропии, интерпретируя разбиения выборки, как дискретные распределения (вероятность отнесения к кластеру равна доле объектов в нём). Индекс $MI$ определяется как взаимная информация для двух распределений, соответствующих разбиениям выборки на кластеры. Интуитивно, взаимная информация измеряет долю информации, общей для обоих разбиений: насколько информация об одном из них уменьшает неопределенность относительно другого.\n",
    "\n",
    "Аналогично $\\text{ARI}$ определяется индекс $\\text{AMI}$, позволяющий избавиться от роста индекса $MI$ с увеличением числа классов. Он принимает значения в диапазоне $[0, 1]$. Значения, близкие к нулю, говорят о независимости разбиений, а близкие к единице - об их схожести (совпадении при $\\text{AMI} = 1$).\n",
    "\n",
    "## Гомогенность, полнота, V-мера\n",
    "\n",
    "Формально данные меры также определяются с использованием функций энтропии и условной энтропии, рассматривая разбиения выборки как дискретные распределения:\n",
    "\n",
    "$$\n",
    "h = 1 - \\frac{H(C\\mid K)}{H(C)}, c = 1 - \\frac{H(K\\mid C)}{H(K)}\n",
    "$$\n",
    "\n",
    "здесь $K$ - результат кластеризации, $C$ - истинное разбиение выборки на классы. Таким образом, $h$ измеряет, насколько каждый кластер состоит из объектов одного класса, а $c$ - насколько объекты одного класса относятся к одному кластеру. Эти меры не являются симметричными. Обе величины принимают значения в диапазоне $[0, 1]$, и большие значения соответствуют более точной кластеризации. Эти меры не являются нормализованными, как $\\text{ARI}$ или $\\text{AMI}$, и поэтому зависят от числа кластеров. Случайная кластеризация не будет давать нулевые показатели при большом числе классов и малом числе объектов. В этих случаях предпочтительнее использовать $\\text{ARI}$. Однако при числе объектов более 1000 и числе кластеров менее 10 данная проблема не так явно выражена и может быть проигнорирована.\n",
    "\n",
    "Для учёта обеих величин $h$ и $c$ одновременно вводится $V$-мера, как их среднее гармоническое:\n",
    "\n",
    "$$\n",
    "v = 2\\frac{hc}{h+c}\n",
    "$$\n",
    "\n",
    "Она является симметричной и показывает, насколько две кластеризации схожи между собой.\n",
    "\n",
    "## Cилуэт\n",
    "\n",
    "В отличие от описанных выше метрик, данный коэффициент не предполагает знания истинных меток объектов, и позволяет оценить качество кластеризации, используя только саму (неразмеченную) выборку и результат кластеризации. Сначала силуэт определяется отдельно для каждого объекта. Обозначим через $a$ - среднее расстояние от данного объекта до объектов из того же кластера, через $b$ - среднее расстояние от данного объекта до объектов из ближайшего кластера (отличного от того, в котором лежит сам объект). Тогда силуэтом данного объекта называется величина:\n",
    "\n",
    "$$\n",
    "s = \\frac{b - a}{\\max(a, b)}\n",
    "$$\n",
    "\n",
    "Силуэтом выборки называется средняя величина силуэта объектов данной выборки. Таким образом, силуэт показывает, насколько среднее расстояние до объектов своего кластера отличается от среднего расстояния до объектов других кластеров. Данная величина лежит в диапазоне $[-1, 1]$. Значения, близкие к -1, соответствуют плохим (разрозненным) кластеризациям, значения, близкие к нулю, говорят о том, что кластеры пересекаются и накладываются друг на друга, значения, близкие к 1, соответствуют \"плотным\" четко выделенным кластерам. Таким образом, чем больше силуэт, тем более четко выделены кластеры, и они представляют собой компактные, плотно сгруппированные облака точек.\n",
    "\n",
    "С помощью силуэта можно выбирать оптимальное число кластеров $k$ (если оно заранее неизвестно) - выбирается число кластеров, максимизирующее значение силуэта. В отличие от предыдущих метрик, силуэт зависит от формы кластеров, и достигает больших значений на более выпуклых кластерах, получаемых с помощью алгоритмов, основанных на восстановлении плотности распределения.\n",
    "\n",
    "### Пример\n",
    "\n",
    "Давайте посмотрим на эти метрики для `KMeans` и других алгоритмов кластеризации, запущенных на данных рукописных цифр MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, SpectralClustering\n",
    "\n",
    "\n",
    "data = datasets.load_digits()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "algorithms = []\n",
    "algorithms.append(KMeans(n_clusters=10, random_state=7))\n",
    "algorithms.append(AffinityPropagation())\n",
    "algorithms.append(SpectralClustering(n_clusters=10, random_state=7, affinity='nearest_neighbors'))\n",
    "algorithms.append(AgglomerativeClustering(n_clusters=10))\n",
    "\n",
    "data = []\n",
    "for algo in algorithms:\n",
    "    algo.fit(X)\n",
    "    data.append(({\n",
    "        'ARI': metrics.adjusted_rand_score(y, algo.labels_),\n",
    "        'AMI': metrics.adjusted_mutual_info_score(y, algo.labels_, average_method='arithmetic'),\n",
    "        'Homogenity': metrics.homogeneity_score(y, algo.labels_),\n",
    "        'Completeness': metrics.completeness_score(y, algo.labels_),\n",
    "        'V-measure': metrics.v_measure_score(y, algo.labels_),\n",
    "        'Silhouette': metrics.silhouette_score(X, algo.labels_)\n",
    "    }))\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    data=data,\n",
    "    columns=['ARI', 'AMI', 'Homogenity', 'Completeness', 'V-measure', 'Silhouette'],\n",
    "    index=['K-means', 'Affinity', 'Spectral', 'Agglomerative']\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полезные источники\n",
    "\n",
    "1. [статья](https://habrahabr.ru/company/io/blog/265089/) \"Как подобрать платье с помощью метода главных компонент\"\n",
    "2. [статья](https://habrahabr.ru/users/lexxo/topics/) \"Как работает метод главных компонент (PCA) на простом примере\"\n",
    "3. [статья](https://habrahabr.ru/post/321216/) \"Интересные алгоритмы кластеризации, часть первая: Affinity propagation\"\n",
    "4. [статья](https://habrahabr.ru/post/322034/) \"Интересные алгоритмы кластеризации, часть вторая: DBSCAN\"\n",
    "5. [обзор](http://scikit-learn.org/stable/modules/clustering.html) методов кластеризации в пакете scikit-learn (англ.)\n",
    "6. [Q&A](http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) Разбор PCA с интуицией и примерами (англ.)\n",
    "7. [тетрадка](https://github.com/diefimov/MTH594_MachineLearning/blob/master/ipython/Lecture10.ipynb) про k-Means и EM-алгоритм в курсе Дмитрия Ефимова (англ.)\n",
    "8. [конспект](https://github.com/esokolov/ml-course-hse/blob/master/2016-fall/lecture-notes/lecture12-unsupervised.pdf) \"Обучение без учителя\" в курсе Евгения Соколова\n",
    "\n",
    "Материал собран на основе соответствующего занятия в курсе [mlcourse.ai](https://mlcourse.ai/), исходный материал можно найти на [GitHub](https://github.com/Yorko/mlcourse.ai)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
